{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_comentario_total = pd.read_table('train.tsv',usecols=[0,2,3])\n",
    "#base_comentario_predicao = pd.read_table('train.tsv',usecols=[0,2])\n",
    "\n",
    "base_comentarios_treinamento,base_comentarios_teste=train_test_split(base_comentario_total,test_size=0.3, random_state=42)\n",
    "\n",
    "tamanho_treinamento=len(base_comentarios_treinamento)\n",
    "tamanho_teste=len(base_comentarios_teste)\n",
    "\n",
    "base_comentarios_treinamento_lista=[]\n",
    "for i in range(0,7000):\n",
    "    base_comentarios_treinamento_lista.append([str(base_comentarios_treinamento.values[i,j]) for  j in range(1,3)])\n",
    "\n",
    "\n",
    "base_comentarios_teste_lista=[]\n",
    "for i in range(0,3000):\n",
    "    base_comentarios_teste_lista.append([str(base_comentarios_teste.values[i,j]) for  j in range(1,3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordscompleto = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def RetirarStopWordsRadical(texto):\n",
    "    radical=nltk.stem.SnowballStemmer('english')\n",
    "    frasesSemRadical = []\n",
    "    for (Phrase,Sentiment) in texto:\n",
    "        FraseLimpa=[str(radical.stem(p)) for p in Phrase.split()  if p not in stopwordscompleto]\n",
    "        frasesSemRadical.append((FraseLimpa,Sentiment))\n",
    "    return frasesSemRadical\n",
    "\n",
    "\n",
    "FrasesTreinamentoSemRadical =RetirarStopWordsRadical(base_comentarios_treinamento_lista)\n",
    "FrasestesteSemRadical =RetirarStopWordsRadical(base_comentarios_teste_lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linha=[]\n",
    "linhacomdez=[]\n",
    "linha2=[]\n",
    "linhacomdez2=[]\n",
    "for i in range(len(FrasesTreinamentoSemRadical)):\n",
    "    linha=FrasesTreinamentoSemRadical[i][0]\n",
    "    tamanho=len(linha)\n",
    "    if tamanho==10:\n",
    "        linhacomdez.append(linha)\n",
    "        linha2=FrasesTreinamentoSemRadical[i][1]\n",
    "        linhacomdez2.append(linha2)\n",
    "\n",
    "\n",
    "###########################BASE DE TESTE######################\n",
    "linhaTeste=[]\n",
    "linhacomdezTeste=[]\n",
    "linha2teste=[]\n",
    "linhacomdez2teste=[]\n",
    "for i in range(len(FrasestesteSemRadical)):\n",
    "    linhaTeste=FrasestesteSemRadical[i][0]\n",
    "    tamanho=len(linhaTeste)\n",
    "    if tamanho==10:\n",
    "        linhacomdezTeste.append(linhaTeste)\n",
    "        linha2teste=FrasesTreinamentoSemRadical[i][1]\n",
    "        linhacomdez2teste.append(linha2teste)\n",
    "\n",
    "\n",
    "###array\n",
    "entradaTreinamento=np.array(linhacomdez)\n",
    "#entradaTreinamentoorigial=np.array(linhacomdez,dtype=str)\n",
    "saidaTreinamento=np.array(linhacomdez2)\n",
    "#entradaTreinamentoorigial=entradaTreinamento\n",
    "entradaTeste=np.array(linhacomdezTeste)\n",
    "#entradaTesteOriginal=np.array(linhacomdezTeste)\n",
    "saidaTeste=np.array(linhacomdez2teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linha=0\n",
    "coluna=0\n",
    "entradaDePara=np.array\n",
    "numero=0\n",
    "qtlinhasmatrix=len(entradaTreinamento)\n",
    "qtlinhasmatrixTeste=len(entradaTeste)\n",
    "for linha in range(qtlinhasmatrix):\n",
    "    for coluna in range(10):\n",
    "            numero=numero+1\n",
    "            palavra=entradaTreinamento[linha,coluna]\n",
    "            linhaloc=0\n",
    "            colunaloc=0\n",
    "            for linhaloc in range(qtlinhasmatrix): #localizando na matrix treinamento\n",
    "                for colunaloc in range(10):\n",
    "                    if entradaTreinamento[linhaloc,colunaloc]==palavra:\n",
    "                        entradaTreinamento[linhaloc,colunaloc]=numero\n",
    "                        linhamatrixteste=0\n",
    "                        colunamatrixteste=0\n",
    "                        for linhamatrixteste in range(qtlinhasmatrixTeste): #localizando na matrix teste\n",
    "                            for colunamatrixteste in range(10):\n",
    "                                if entradaTeste[linhamatrixteste,colunamatrixteste]==palavra:\n",
    "                                    entradaTeste[linhamatrixteste,colunamatrixteste]=numero\n",
    "                                    \n",
    "\n",
    "########\n",
    "#try:\n",
    "#    print(int(entradaTeste[0,0]))\n",
    "#except:\n",
    "#    print('n')\n",
    "\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "#para as palavras n√£o localizadas na entrada teste\n",
    "linha=0\n",
    "coluna=0\n",
    "numero=numero+1\n",
    "inteiro=''\n",
    "qtlinhasmatrixTeste=len(entradaTeste)\n",
    "for linha in range(qtlinhasmatrixTeste):\n",
    "    for coluna in range(10):\n",
    "        numero=numero+1\n",
    "        palavra=entradaTeste[linha,coluna]\n",
    "        try:\n",
    "            palavra=int(palavra)\n",
    "            inteiro='S'\n",
    "        except:\n",
    "            inteiro='N'\n",
    "        if inteiro=='N':\n",
    "            linhamatrixteste=0\n",
    "            colunamatrixteste=0\n",
    "            for linhamatrixteste in range(qtlinhasmatrixTeste):\n",
    "                for colunamatrixteste in range(10):\n",
    "                    if entradaTeste[linhamatrixteste,colunamatrixteste]==palavra:\n",
    "                        entradaTeste[linhamatrixteste,colunamatrixteste]=int(numero)\n",
    "                        \n",
    "                    \n",
    "\n",
    "\n",
    "entradaTreinamento=entradaTreinamento.astype('int32')\n",
    "entradaTeste=entradaTeste.astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.40730856\n",
      "Iteration 2, loss = 2.26360015\n",
      "Iteration 3, loss = 2.14885216\n",
      "Iteration 4, loss = 2.05120438\n",
      "Iteration 5, loss = 1.98062723\n",
      "Iteration 6, loss = 1.92530763\n",
      "Iteration 7, loss = 1.86971816\n",
      "Iteration 8, loss = 1.83237738\n",
      "Iteration 9, loss = 1.79655299\n",
      "Iteration 10, loss = 1.76363690\n",
      "Iteration 11, loss = 1.74451712\n",
      "Iteration 12, loss = 1.72034527\n",
      "Iteration 13, loss = 1.70271382\n",
      "Iteration 14, loss = 1.68178734\n",
      "Iteration 15, loss = 1.66919785\n",
      "Iteration 16, loss = 1.65049114\n",
      "Iteration 17, loss = 1.64520888\n",
      "Iteration 18, loss = 1.63507678\n",
      "Iteration 19, loss = 1.62452531\n",
      "Iteration 20, loss = 1.61427977\n",
      "Iteration 21, loss = 1.60502321\n",
      "Iteration 22, loss = 1.59865157\n",
      "Iteration 23, loss = 1.59385704\n",
      "Iteration 24, loss = 1.59066631\n",
      "Iteration 25, loss = 1.58458823\n",
      "Iteration 26, loss = 1.57977227\n",
      "Iteration 27, loss = 1.57320952\n",
      "Iteration 28, loss = 1.56548089\n",
      "Iteration 29, loss = 1.55902369\n",
      "Iteration 30, loss = 1.55130151\n",
      "Iteration 31, loss = 1.54229287\n",
      "Iteration 32, loss = 1.53494904\n",
      "Iteration 33, loss = 1.53010892\n",
      "Iteration 34, loss = 1.52297178\n",
      "Iteration 35, loss = 1.51686562\n",
      "Iteration 36, loss = 1.51497081\n",
      "Iteration 37, loss = 1.51158660\n",
      "Iteration 38, loss = 1.50400256\n",
      "Iteration 39, loss = 1.49785670\n",
      "Iteration 40, loss = 1.49248897\n",
      "Iteration 41, loss = 1.49153035\n",
      "Iteration 42, loss = 1.48770707\n",
      "Iteration 43, loss = 1.48514522\n",
      "Iteration 44, loss = 1.47872831\n",
      "Iteration 45, loss = 1.47420897\n",
      "Iteration 46, loss = 1.47090530\n",
      "Iteration 47, loss = 1.46531383\n",
      "Iteration 48, loss = 1.46382150\n",
      "Iteration 49, loss = 1.45957642\n",
      "Iteration 50, loss = 1.45082253\n",
      "Iteration 51, loss = 1.44459098\n",
      "Iteration 52, loss = 1.44326436\n",
      "Iteration 53, loss = 1.44231919\n",
      "Iteration 54, loss = 1.43550599\n",
      "Iteration 55, loss = 1.43213042\n",
      "Iteration 56, loss = 1.42852882\n",
      "Iteration 57, loss = 1.42187263\n",
      "Iteration 58, loss = 1.41847998\n",
      "Iteration 59, loss = 1.41606601\n",
      "Iteration 60, loss = 1.41450413\n",
      "Iteration 61, loss = 1.41208603\n",
      "Iteration 62, loss = 1.40954866\n",
      "Iteration 63, loss = 1.40680912\n",
      "Iteration 64, loss = 1.40322678\n",
      "Iteration 65, loss = 1.40079865\n",
      "Iteration 66, loss = 1.39575741\n",
      "Iteration 67, loss = 1.39135570\n",
      "Iteration 68, loss = 1.38704398\n",
      "Iteration 69, loss = 1.38373790\n",
      "Iteration 70, loss = 1.38047103\n",
      "Iteration 71, loss = 1.37757256\n",
      "Iteration 72, loss = 1.37475626\n",
      "Iteration 73, loss = 1.37234738\n",
      "Iteration 74, loss = 1.36960614\n",
      "Iteration 75, loss = 1.36686454\n",
      "Iteration 76, loss = 1.36241588\n",
      "Iteration 77, loss = 1.35950973\n",
      "Iteration 78, loss = 1.35678694\n",
      "Iteration 79, loss = 1.35380858\n",
      "Iteration 80, loss = 1.35175229\n",
      "Iteration 81, loss = 1.34905363\n",
      "Iteration 82, loss = 1.34665109\n",
      "Iteration 83, loss = 1.34320653\n",
      "Iteration 84, loss = 1.34113092\n",
      "Iteration 85, loss = 1.33837434\n",
      "Iteration 86, loss = 1.33725976\n",
      "Iteration 87, loss = 1.33306087\n",
      "Iteration 88, loss = 1.33077661\n",
      "Iteration 89, loss = 1.32573352\n",
      "Iteration 90, loss = 1.32223341\n",
      "Iteration 91, loss = 1.31985025\n",
      "Iteration 92, loss = 1.31711712\n",
      "Iteration 93, loss = 1.31537560\n",
      "Iteration 94, loss = 1.31392110\n",
      "Iteration 95, loss = 1.31141124\n",
      "Iteration 96, loss = 1.30846095\n",
      "Iteration 97, loss = 1.30516790\n",
      "Iteration 98, loss = 1.30352330\n",
      "Iteration 99, loss = 1.30086619\n",
      "Iteration 100, loss = 1.29869353\n",
      "Iteration 101, loss = 1.29629469\n",
      "Iteration 102, loss = 1.29446889\n",
      "Iteration 103, loss = 1.29200376\n",
      "Iteration 104, loss = 1.29235219\n",
      "Iteration 105, loss = 1.28803663\n",
      "Iteration 106, loss = 1.28678133\n",
      "Iteration 107, loss = 1.28371497\n",
      "Iteration 108, loss = 1.28163866\n",
      "Iteration 109, loss = 1.27808075\n",
      "Iteration 110, loss = 1.27770100\n",
      "Iteration 111, loss = 1.27392676\n",
      "Iteration 112, loss = 1.27250620\n",
      "Iteration 113, loss = 1.27002746\n",
      "Iteration 114, loss = 1.26807790\n",
      "Iteration 115, loss = 1.26648708\n",
      "Iteration 116, loss = 1.26412409\n",
      "Iteration 117, loss = 1.26202833\n",
      "Iteration 118, loss = 1.25914624\n",
      "Iteration 119, loss = 1.25717305\n",
      "Iteration 120, loss = 1.25550159\n",
      "Iteration 121, loss = 1.25351800\n",
      "Iteration 122, loss = 1.25176050\n",
      "Iteration 123, loss = 1.25001675\n",
      "Iteration 124, loss = 1.24804144\n",
      "Iteration 125, loss = 1.24624702\n",
      "Iteration 126, loss = 1.24454495\n",
      "Iteration 127, loss = 1.24271104\n",
      "Iteration 128, loss = 1.24093653\n",
      "Iteration 129, loss = 1.23925354\n",
      "Iteration 130, loss = 1.23746223\n",
      "Iteration 131, loss = 1.23404192\n",
      "Iteration 132, loss = 1.23235879\n",
      "Iteration 133, loss = 1.23063638\n",
      "Iteration 134, loss = 1.22890103\n",
      "Iteration 135, loss = 1.22744397\n",
      "Iteration 136, loss = 1.22379501\n",
      "Iteration 137, loss = 1.22141910\n",
      "Iteration 138, loss = 1.21948047\n",
      "Iteration 139, loss = 1.21771396\n",
      "Iteration 140, loss = 1.21445858\n",
      "Iteration 141, loss = 1.21242117\n",
      "Iteration 142, loss = 1.21026339\n",
      "Iteration 143, loss = 1.20732963\n",
      "Iteration 144, loss = 1.20471481\n",
      "Iteration 145, loss = 1.20317200\n",
      "Iteration 146, loss = 1.20216097\n",
      "Iteration 147, loss = 1.20074317\n",
      "Iteration 148, loss = 1.19917053\n",
      "Iteration 149, loss = 1.19785701\n",
      "Iteration 150, loss = 1.19582345\n",
      "Iteration 151, loss = 1.19401639\n",
      "Iteration 152, loss = 1.19288121\n",
      "Iteration 153, loss = 1.19063959\n",
      "Iteration 154, loss = 1.18868931\n",
      "Iteration 155, loss = 1.18723111\n",
      "Iteration 156, loss = 1.18610292\n",
      "Iteration 157, loss = 1.18426736\n",
      "Iteration 158, loss = 1.18228618\n",
      "Iteration 159, loss = 1.18064974\n",
      "Iteration 160, loss = 1.17927053\n",
      "Iteration 161, loss = 1.17782374\n",
      "Iteration 162, loss = 1.17655426\n",
      "Iteration 163, loss = 1.17510652\n",
      "Iteration 164, loss = 1.17343898\n",
      "Iteration 165, loss = 1.17193722\n",
      "Iteration 166, loss = 1.17051930\n",
      "Iteration 167, loss = 1.16913986\n",
      "Iteration 168, loss = 1.16774109\n",
      "Iteration 169, loss = 1.16636325\n",
      "Iteration 170, loss = 1.16498967\n",
      "Iteration 171, loss = 1.16358293\n",
      "Iteration 172, loss = 1.16217809\n",
      "Iteration 173, loss = 1.16079305\n",
      "Iteration 174, loss = 1.15941964\n",
      "Iteration 175, loss = 1.15804984\n",
      "Iteration 176, loss = 1.15666845\n",
      "Iteration 177, loss = 1.15520128\n",
      "Iteration 178, loss = 1.15403215\n",
      "Iteration 179, loss = 1.15218560\n",
      "Iteration 180, loss = 1.15058944\n",
      "Iteration 181, loss = 1.14878089\n",
      "Iteration 182, loss = 1.14745601\n",
      "Iteration 183, loss = 1.14615925\n",
      "Iteration 184, loss = 1.14486883\n",
      "Iteration 185, loss = 1.14358262\n",
      "Iteration 186, loss = 1.14241916\n",
      "Iteration 187, loss = 1.14113555\n",
      "Iteration 188, loss = 1.13976007\n",
      "Iteration 189, loss = 1.13850054\n",
      "Iteration 190, loss = 1.13725093\n",
      "Iteration 191, loss = 1.13600719\n",
      "Iteration 192, loss = 1.13476376\n",
      "Iteration 193, loss = 1.13352201\n",
      "Iteration 194, loss = 1.13228689\n",
      "Iteration 195, loss = 1.13105989\n",
      "Iteration 196, loss = 1.12984040\n",
      "Iteration 197, loss = 1.12862734\n",
      "Iteration 198, loss = 1.12742003\n",
      "Iteration 199, loss = 1.12621814\n",
      "Iteration 200, loss = 1.12502130\n",
      "Iteration 201, loss = 1.12382926\n",
      "Iteration 202, loss = 1.12264202\n",
      "Iteration 203, loss = 1.12145958\n",
      "Iteration 204, loss = 1.12028220\n",
      "Iteration 205, loss = 1.11911129\n",
      "Iteration 206, loss = 1.11794988\n",
      "Iteration 207, loss = 1.11681725\n",
      "Iteration 208, loss = 1.11574194\n",
      "Iteration 209, loss = 1.11507938\n",
      "Iteration 210, loss = 1.11408051\n",
      "Iteration 211, loss = 1.11294520\n",
      "Iteration 212, loss = 1.11163472\n",
      "Iteration 213, loss = 1.11045097\n",
      "Iteration 214, loss = 1.10985251\n",
      "Iteration 215, loss = 1.11115406\n",
      "Iteration 216, loss = 1.11003710\n",
      "Iteration 217, loss = 1.10512641\n",
      "Iteration 218, loss = 1.10489550\n",
      "Iteration 219, loss = 1.10406997\n",
      "Iteration 220, loss = 1.10292832\n",
      "Iteration 221, loss = 1.10096344\n",
      "Iteration 222, loss = 1.09827388\n",
      "Iteration 223, loss = 1.09785658\n",
      "Iteration 224, loss = 1.09659172\n",
      "Iteration 225, loss = 1.09278030\n",
      "Iteration 226, loss = 1.09197824\n",
      "Iteration 227, loss = 1.09016396\n",
      "Iteration 228, loss = 1.08903604\n",
      "Iteration 229, loss = 1.08809457\n",
      "Iteration 230, loss = 1.08693905\n",
      "Iteration 231, loss = 1.08587198\n",
      "Iteration 232, loss = 1.08482145\n",
      "Iteration 233, loss = 1.08378126\n",
      "Iteration 234, loss = 1.08261702\n",
      "Iteration 235, loss = 1.08141652\n",
      "Iteration 236, loss = 1.08032795\n",
      "Iteration 237, loss = 1.07927979\n",
      "Iteration 238, loss = 1.07807931\n",
      "Iteration 239, loss = 1.07810225\n",
      "Iteration 240, loss = 1.07625883\n",
      "Iteration 241, loss = 1.07527871\n",
      "Iteration 242, loss = 1.07457154\n",
      "Iteration 243, loss = 1.07356285\n",
      "Iteration 244, loss = 1.07309833\n",
      "Iteration 245, loss = 1.07308764\n",
      "Iteration 246, loss = 1.07290246\n",
      "Iteration 247, loss = 1.07191284\n",
      "Iteration 248, loss = 1.07194691\n",
      "Iteration 249, loss = 1.06946096\n",
      "Iteration 250, loss = 1.06971827\n",
      "Iteration 251, loss = 1.06687368\n",
      "Iteration 252, loss = 1.06598413\n",
      "Iteration 253, loss = 1.06434037\n",
      "Iteration 254, loss = 1.06355042\n",
      "Iteration 255, loss = 1.06258661\n",
      "Iteration 256, loss = 1.06147619\n",
      "Iteration 257, loss = 1.06039393\n",
      "Iteration 258, loss = 1.05927551\n",
      "Iteration 259, loss = 1.05636379\n",
      "Iteration 260, loss = 1.05470776\n",
      "Iteration 261, loss = 1.05975009\n",
      "Iteration 262, loss = 1.05037777\n",
      "Iteration 263, loss = 1.05308897\n",
      "Iteration 264, loss = 1.05268500\n",
      "Iteration 265, loss = 1.05176855\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "redeneural=MLPClassifier(verbose=True,\n",
    "                         max_iter=1000,\n",
    "                         tol=0.00001,\n",
    "                         activation='tanh',\n",
    "                         learning_rate_init=0.001)\n",
    "\n",
    "#redeneural.fit(entradas,saidas)\n",
    "\n",
    "\n",
    "redeneural.fit(entradaTreinamento,saidaTreinamento)\n",
    "\n",
    "\n",
    "predicao=redeneural.predict(entradaTeste)\n",
    "\n",
    "predicao=predicao.astype('int32')\n",
    "#predicao=predicao.reshape(1,-1)\n",
    "\n",
    "saidaTeste=saidaTeste.astype('int32')\n",
    "#saidaTeste=saidaTeste.reshape(1,-1)\n",
    "\n",
    "#calcular acuracia\n",
    "linha=0\n",
    "coluna=0\n",
    "tamanho=len(predicao)\n",
    "igual=''\n",
    "igual=0\n",
    "for linha in range(tamanho):\n",
    "    if predicao[linha]==saidaTeste[linha]:\n",
    "        igual=igual+1\n",
    "\n",
    "acuracia=igual/tamanho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.391304347826087"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acuracia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
